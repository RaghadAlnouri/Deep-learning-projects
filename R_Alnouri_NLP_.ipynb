{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R. Alnouri - NLP .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghadAlnouri/Raghad/blob/master/R_Alnouri_NLP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yz0AbWFaxbE",
        "colab_type": "text"
      },
      "source": [
        "# NLP part 1 homework\n",
        "\n",
        "We've introduced classic techniques to classify the different posts in the 20newsgroup dataset (Basics.ipynb notebook). The goal is to use one of the classification techniques using embeddings and see if you can outperform the random forest applied to word counts.\n",
        "\n",
        "Criteria:\n",
        "- Can you outperform the random forest on word counts?\n",
        "- Use one of the embedding-based methods to build features for documents, and train a classifier on top of it.\n",
        "- Describe what kind of mistakes your model makes on the test set. Look at errors in the test set classification. Can you come up with a reason why the model didn't get it? Do you have a hypothesis on how to fix this? Write a short paragraph/few bullet points about the errors that seem to occur and what you would try if you had to fix them.\n",
        "- Don't build vocabulary using the test set!!!\n",
        "\n",
        "Some tips:\n",
        "- Even if using embedding-based models, preprocessing can help.\n",
        "- Look at the data, can you see anything embeddings will have a hard time with?\n",
        "- How are you handling Out-of-Vocabulary words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYzBSF9ErWVO",
        "colab_type": "text"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhKdRbZiawvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFgRR_A0de6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = newsgroups_train['data'], newsgroups_train['target']\n",
        "X_test, y_test = newsgroups_test['data'], newsgroups_test['target']\n",
        "target_names = newsgroups_train['target_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8QINsFIpU1Y",
        "colab_type": "text"
      },
      "source": [
        "## Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kczJOFZAreVr",
        "colab_type": "code",
        "outputId": "4c7a8900-06de-4275-95b0-0469a8b75e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(\"number of classes:\", len(target_names))\n",
        "print(\"classes:\\n\", \", \".join(target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of classes: 20\n",
            "classes:\n",
            " alt.atheism, comp.graphics, comp.os.ms-windows.misc, comp.sys.ibm.pc.hardware, comp.sys.mac.hardware, comp.windows.x, misc.forsale, rec.autos, rec.motorcycles, rec.sport.baseball, rec.sport.hockey, sci.crypt, sci.electronics, sci.med, sci.space, soc.religion.christian, talk.politics.guns, talk.politics.mideast, talk.politics.misc, talk.religion.misc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhUGVZ7qawyZ",
        "colab_type": "code",
        "outputId": "8ffaa8bc-31d6-4bd1-8928-20bb136f93a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# select random samples from data and print them\n",
        "indices = np.random.randint(0, len(X_train), 3)\n",
        "for i in indices:\n",
        "  print(\"class: \", target_names[y_train[i]])\n",
        "  print(\"example:\\n\", X_train[i])\n",
        "  print('*'*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class:  comp.windows.x\n",
            "example:\n",
            " From: hsteve@carina.unm.edu ()\n",
            "Subject: XTranslateCoord. Problem\n",
            "Organization: University of New Mexico, Albuquerque\n",
            "Lines: 13\n",
            "NNTP-Posting-Host: carina.unm.edu\n",
            "\n",
            "It seems like XTranslateCoord. doesn't work the way I expecting it.  Right\n",
            "after performs a XMoveWindow, I want to know the absolute window position\n",
            "with respect to the root window.  To get this info. I do a XTranslateCoordinates\n",
            "but the abs_x, and abs_y aren't right?  Does anybody know of a way to find \n",
            "out this information?\n",
            "\n",
            "Thanks, please e-mail to hsteve@carina.unm.edu if it's possible\n",
            "\n",
            "-- \n",
            "    _---_     Steve  \n",
            "   / o o \\    hsteve@hydra.unm.edu, hsteve@carina.unm.edu\n",
            "  | \\___/ |   \n",
            "              Just say NO to VMS!!\n",
            "\n",
            "**************************************************\n",
            "class:  comp.os.ms-windows.misc\n",
            "example:\n",
            " From: pmw0@ns1.cc.lehigh.edu (PHILLIP MICHAEL WILLIAMS)\n",
            "Subject: X Windows for windows\n",
            "Organization: Lehigh University\n",
            "Lines: 7\n",
            "\n",
            "Are there any X window servers that can run under MS-Windows??  I only know of\n",
            "Deskview but have not seen it in action.  Are there any others??\n",
            "\n",
            "Thanks in advance.\n",
            "\n",
            "Phil\n",
            "pmw0@Lehigh.edu\n",
            "\n",
            "**************************************************\n",
            "class:  rec.autos\n",
            "example:\n",
            " From: rjwade@rainbow.ecn.purdue.edu (Robert J. Wade)\n",
            "Subject: Re: Most bang for between $13,000 and $16,000\n",
            "Organization: Purdue University Engineering Computer Network\n",
            "Distribution: na\n",
            "Lines: 18\n",
            "\n",
            "In article <33759@oasys.dt.navy.mil> tobias@oasys.dt.navy.mil (Steve Tobias) writes:\n",
            ">In rec.autos, CPKJP@vm.cc.latech.edu (Kevin Parker) writes:\n",
            ">>   I'd like to get some feedback on a car with most bang for the buck in the\n",
            ">>$13000 to 16,000 price range. I'm looking for a car with enough civility to be\n",
            ">>driven every day, or even on long trips, but when I hit the gas, I want to feel\n",
            ">>some acceleration.  Handling is important also, as are reliability and pretty\n",
            ">>low maintenance costs.  A stylish appearance is nice, but I don't want a car\n",
            ">>that is all show and not much go.  Even though many of the imports are fast, I\n",
            ">>don't really want a turbo, and I never have cared for the song sung by a four\n",
            ">>clyinder.  I'd prefer a v6 or v8 for the engine.  If you have any suggestions,\n",
            ">>Kevin Parker\n",
            ">\n",
            ">     There's only one car that really fits your needs. It's spelled:\n",
            ">\n",
            ">\t\t\t  5.0 LITER MUSTANG\n",
            "\n",
            "\n",
            "not!  sorry, he said cvility, long trips, reliability, and low maintenance cost!\n",
            "\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYpYP3FGhopI",
        "colab_type": "text"
      },
      "source": [
        "## load google word2vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3878189e-b0bc-4306-ead6-5720f0422aeb",
        "id": "uleFGfvZsT4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "from gensim.models import KeyedVectors\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "word2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-10 17:48:19--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.178.221\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.178.221|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  48.6MB/s    in 33s     \n",
            "\n",
            "2020-03-10 17:48:52 (47.5 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n",
            "gzip: GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TippYrar28U",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bhKz72IxDZb",
        "colab_type": "code",
        "outputId": "d23ac3b9-08c8-4ddb-9f4e-e0e7e94b1565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# install missing package\n",
        "!pip install unidecode\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzfTyJ4raw37",
        "colab_type": "code",
        "outputId": "5f8a08dd-94a5-4f5c-e9f7-6487fbe60fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# imports\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from unidecode import unidecode\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65aiwJdruRYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing functions\n",
        "\n",
        "def clean(s):\n",
        "  s = s.lower()\n",
        "  s = unidecode(s) # remove accents\n",
        "  lines = s.split('\\n')\n",
        "  lines = [x for x in lines if len(x) != 0]\n",
        "  lines = [x for x in lines\n",
        "           if not (x.startswith('from') or x.startswith('lines') or x.startswith('organization'))]\n",
        "  return '\\n'.join(lines)\n",
        "\n",
        "stopwords = set(nltk_stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "alphanum = set('qwertyuiopasdfghjklzxcvbnm1234567890')\n",
        "\n",
        "def tokenize(s):\n",
        "  tokens = nltk.tokenize.word_tokenize(s)\n",
        "  \n",
        "  # remove stopwords\n",
        "  tokens = [word for word in tokens if word not in stopwords]\n",
        "  \n",
        "  # stem or lemmatize words \"Didn't improve the performance\"\n",
        "  # tokens = [stemmer.stem(word) for word in tokens]\n",
        "  # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  \n",
        "  # remove non alpha-numeric tokens\n",
        "  tokens = [word for word in tokens\n",
        "            if all(char in alphanum for char in word)]\n",
        "\n",
        "  # tokens = [word for word in tokens\n",
        "  #           if not all(char not in alphanum for char in word)]\n",
        "  return tokens\n",
        "\n",
        "# takes some time...\n",
        "X_train_clean = [clean(x) for x in X_train]\n",
        "X_test_clean = [clean(x) for x in X_test]\n",
        "\n",
        "X_train_tok = [tokenize(x) for x in X_train_clean]\n",
        "X_test_tok = [tokenize(x) for x in X_test_clean]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiLsQs0Z4Mk_",
        "colab_type": "code",
        "outputId": "db6b2802-de1d-40c5-e1c2-5e9ca460321b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "# take a look at the data\n",
        "print(\"original:\\n\", X_train[0])\n",
        "print(\"*\" * 50)\n",
        "print(\"clean:\\n\", X_train_clean[0])\n",
        "print(\"*\" * 50)\n",
        "print(\"tokenized:\\n\", X_train_tok[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            " From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**************************************************\n",
            "clean:\n",
            " subject: what car is this!?\n",
            "nntp-posting-host: rac3.wam.umd.edu\n",
            " i was wondering if anyone out there could enlighten me on this car i saw\n",
            "the other day. it was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. it was called a bricklin. the doors were really small. in addition,\n",
            "the front bumper was separate from the rest of the body. this is \n",
            "all i know. if anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "thanks,\n",
            "- il\n",
            "   ---- brought to you by your neighborhood lerxst ----\n",
            "**************************************************\n",
            "tokenized:\n",
            " ['subject', 'car', 'wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sports', 'car', 'looked', 'late', 'early', '70s', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'thanks', 'il', 'brought', 'neighborhood', 'lerxst']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl_AOFvf5i53",
        "colab_type": "text"
      },
      "source": [
        "## Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EG6cdp75hx2",
        "colab_type": "code",
        "outputId": "c3ca4d0e-1dfe-42d5-85fb-425ca2e7bdd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# list of all words\n",
        "all_words = [word for sent in X_train_tok for word in sent]\n",
        "\n",
        "# put them in a set\n",
        "all_words_set = set(all_words)\n",
        "print(\"# all words =\", len(all_words_set))\n",
        "\n",
        "# count words in all_words\n",
        "from collections import Counter\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# choose only words with counts >= MIN_COUNT in the text. The list is sorted by word count\n",
        "MIN_COUNT = 20\n",
        "vocab_list = [word for word, count in word_counts.most_common() if count >= MIN_COUNT]\n",
        "\n",
        "# add OOV token to vocab_list, which will have index 0 \"Handelling out of vocabulary words\"\n",
        "vocab_list = ['OOV'] + vocab_list\n",
        "print(\"# words in vocabulary =\", len(vocab_list))\n",
        "\n",
        "# create a vocabulary and inverse-vocabulary\n",
        "from collections import OrderedDict\n",
        "vocab_ind = OrderedDict([(word,i) for i,word in enumerate(vocab_list)])\n",
        "ind_vocab = vocab_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# all words = 90927\n",
            "# words in vocabulary = 9861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmFRFUJM-F6j",
        "colab_type": "code",
        "outputId": "be67a584-1513-474b-a992-9fbf3b34c4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# look at first 10 words in our vocab\n",
        "print(list(vocab_ind.items())[:10])\n",
        "print(\", \".join([\"%d:%s\" % (i, word) for i, word in enumerate(ind_vocab[:10])]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('OOV', 0), ('subject', 1), ('would', 2), ('one', 3), ('writes', 4), ('article', 5), ('1', 6), ('x', 7), ('people', 8), ('like', 9)]\n",
            "0:OOV, 1:subject, 2:would, 3:one, 4:writes, 5:article, 6:1, 7:x, 8:people, 9:like\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlAkG1F2-_Ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert train and test words to integers so we can use embeddings\n",
        "\n",
        "# def token_to_int(sent):\n",
        "#   # handle out-of-vocabulary words using OOV token (index 0 in the vocab).\n",
        "#   return [vocab_ind[word] if word in vocab_ind else 0 for word in sent]\n",
        "\n",
        "# def int_to_token(sent):\n",
        "#   return [ind_vocab[i] for i in sent]\n",
        "\n",
        "# X_train_ind = [token_to_int(sent) for sent in X_train_tok]\n",
        "# X_test_ind = [token_to_int(sent) for sent in X_test_tok]\n",
        "\n",
        "# # look at first example\n",
        "# print(X_train_ind[0])\n",
        "# print(X_test_ind[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s6arg6QBw05",
        "colab_type": "text"
      },
      "source": [
        "## Construct features for data using average word2vec embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e98jYPsoBvvX",
        "colab_type": "code",
        "outputId": "3e937d07-886b-459c-e322-4c78f0185995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def token_to_feat(sent):\n",
        "  # note that we will ignore OOV words from the mean features\n",
        "  feature_array = np.array([word2vec[word] for word in sent if word in word2vec])\n",
        "  return feature_array.mean(axis=0)\n",
        "\n",
        "  X_train_feat = np.array([token_to_feat(sent) for sent in X_train_tok])\n",
        "  X_test_feat = np.array([token_to_feat(sent) for sent in X_test_tok])\n",
        "\n",
        "# take a look at data\n",
        "#print(len(X_train_feat))\n",
        "#print(len(X_test_feat))\n",
        "print(X_train_feat.shape)\n",
        "print(X_test_feat.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 300)\n",
            "(7532, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nobz-xjoGFWJ",
        "colab_type": "text"
      },
      "source": [
        "## Train a classifier on top of word2vec features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGg6i3cjF7X4",
        "colab_type": "code",
        "outputId": "03f1f2cb-d715-4921-ad59-cf45c6cd3f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "inputs = keras.Input(shape=(300,))\n",
        "h1 = Dense(units=256, activation='relu')(inputs)\n",
        "h2 = Dropout(0.5)(h1)\n",
        "h3 = Dense(units=256, activation='relu')(h2)\n",
        "h4 = Dropout(0.5)(h3)\n",
        "outputs = Dense(units=20, activation='softmax')(h4)\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "func_model = keras.Model(inputs, outputs)\n",
        "func_model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
        "                   optimizer=keras.optimizers.Adam(learning_rate),\n",
        "                   metrics=[keras.metrics.sparse_categorical_accuracy])\n",
        "func_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 300)]             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               77056     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 20)                5140      \n",
            "=================================================================\n",
            "Total params: 147,988\n",
            "Trainable params: 147,988\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HJH_2LRGbtw",
        "colab_type": "code",
        "outputId": "24b94475-1710-4cc1-f09b-1cec3c3a7dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = func_model.fit(X_train_feat, y_train, validation_data=(X_test_feat, y_test), batch_size=32, shuffle=True, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11314 samples, validate on 7532 samples\n",
            "Epoch 1/100\n",
            "11314/11314 [==============================] - 1s 116us/sample - loss: 2.1068 - sparse_categorical_accuracy: 0.3056 - val_loss: 1.4138 - val_sparse_categorical_accuracy: 0.5252\n",
            "Epoch 2/100\n",
            "11314/11314 [==============================] - 1s 110us/sample - loss: 1.3277 - sparse_categorical_accuracy: 0.5354 - val_loss: 1.1668 - val_sparse_categorical_accuracy: 0.6257\n",
            "Epoch 3/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 1.1213 - sparse_categorical_accuracy: 0.6197 - val_loss: 1.0590 - val_sparse_categorical_accuracy: 0.6597\n",
            "Epoch 4/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 1.0077 - sparse_categorical_accuracy: 0.6584 - val_loss: 1.0078 - val_sparse_categorical_accuracy: 0.6743\n",
            "Epoch 5/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.9352 - sparse_categorical_accuracy: 0.6876 - val_loss: 0.9907 - val_sparse_categorical_accuracy: 0.6815\n",
            "Epoch 6/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.8842 - sparse_categorical_accuracy: 0.7023 - val_loss: 0.9585 - val_sparse_categorical_accuracy: 0.7042\n",
            "Epoch 7/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.8372 - sparse_categorical_accuracy: 0.7201 - val_loss: 0.9619 - val_sparse_categorical_accuracy: 0.7023\n",
            "Epoch 8/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.7977 - sparse_categorical_accuracy: 0.7318 - val_loss: 0.9421 - val_sparse_categorical_accuracy: 0.7095\n",
            "Epoch 9/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.7671 - sparse_categorical_accuracy: 0.7417 - val_loss: 0.9401 - val_sparse_categorical_accuracy: 0.7111\n",
            "Epoch 10/100\n",
            "11314/11314 [==============================] - 1s 109us/sample - loss: 0.7417 - sparse_categorical_accuracy: 0.7515 - val_loss: 0.9386 - val_sparse_categorical_accuracy: 0.7122\n",
            "Epoch 11/100\n",
            "11314/11314 [==============================] - 1s 102us/sample - loss: 0.7106 - sparse_categorical_accuracy: 0.7656 - val_loss: 0.9388 - val_sparse_categorical_accuracy: 0.7149\n",
            "Epoch 12/100\n",
            "11314/11314 [==============================] - 1s 103us/sample - loss: 0.6874 - sparse_categorical_accuracy: 0.7691 - val_loss: 0.9261 - val_sparse_categorical_accuracy: 0.7183\n",
            "Epoch 13/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.6712 - sparse_categorical_accuracy: 0.7736 - val_loss: 0.9349 - val_sparse_categorical_accuracy: 0.7205\n",
            "Epoch 14/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.6513 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.9244 - val_sparse_categorical_accuracy: 0.7207\n",
            "Epoch 15/100\n",
            "11314/11314 [==============================] - 1s 102us/sample - loss: 0.6230 - sparse_categorical_accuracy: 0.7923 - val_loss: 0.9401 - val_sparse_categorical_accuracy: 0.7181\n",
            "Epoch 16/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.6231 - sparse_categorical_accuracy: 0.7901 - val_loss: 0.9288 - val_sparse_categorical_accuracy: 0.7211\n",
            "Epoch 17/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.5876 - sparse_categorical_accuracy: 0.8040 - val_loss: 0.9445 - val_sparse_categorical_accuracy: 0.7160\n",
            "Epoch 18/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.5883 - sparse_categorical_accuracy: 0.7991 - val_loss: 0.9420 - val_sparse_categorical_accuracy: 0.7192\n",
            "Epoch 19/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.5709 - sparse_categorical_accuracy: 0.8116 - val_loss: 0.9685 - val_sparse_categorical_accuracy: 0.7193\n",
            "Epoch 20/100\n",
            "11314/11314 [==============================] - 1s 101us/sample - loss: 0.5532 - sparse_categorical_accuracy: 0.8123 - val_loss: 0.9326 - val_sparse_categorical_accuracy: 0.7228\n",
            "Epoch 21/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.5375 - sparse_categorical_accuracy: 0.8206 - val_loss: 0.9838 - val_sparse_categorical_accuracy: 0.7191\n",
            "Epoch 22/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.5339 - sparse_categorical_accuracy: 0.8220 - val_loss: 0.9457 - val_sparse_categorical_accuracy: 0.7264\n",
            "Epoch 23/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.5168 - sparse_categorical_accuracy: 0.8288 - val_loss: 0.9652 - val_sparse_categorical_accuracy: 0.7266\n",
            "Epoch 24/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.5099 - sparse_categorical_accuracy: 0.8284 - val_loss: 0.9746 - val_sparse_categorical_accuracy: 0.7248\n",
            "Epoch 25/100\n",
            "11314/11314 [==============================] - 1s 102us/sample - loss: 0.4974 - sparse_categorical_accuracy: 0.8278 - val_loss: 0.9641 - val_sparse_categorical_accuracy: 0.7282\n",
            "Epoch 26/100\n",
            "11314/11314 [==============================] - 1s 103us/sample - loss: 0.4835 - sparse_categorical_accuracy: 0.8379 - val_loss: 0.9657 - val_sparse_categorical_accuracy: 0.7277\n",
            "Epoch 27/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.4657 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.9949 - val_sparse_categorical_accuracy: 0.7256\n",
            "Epoch 28/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.4605 - sparse_categorical_accuracy: 0.8423 - val_loss: 0.9729 - val_sparse_categorical_accuracy: 0.7314\n",
            "Epoch 29/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.4484 - sparse_categorical_accuracy: 0.8467 - val_loss: 1.0018 - val_sparse_categorical_accuracy: 0.7245\n",
            "Epoch 30/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.4430 - sparse_categorical_accuracy: 0.8476 - val_loss: 1.0019 - val_sparse_categorical_accuracy: 0.7269\n",
            "Epoch 31/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.4427 - sparse_categorical_accuracy: 0.8510 - val_loss: 1.0043 - val_sparse_categorical_accuracy: 0.7224\n",
            "Epoch 32/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.4305 - sparse_categorical_accuracy: 0.8513 - val_loss: 1.0065 - val_sparse_categorical_accuracy: 0.7297\n",
            "Epoch 33/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.4232 - sparse_categorical_accuracy: 0.8552 - val_loss: 1.0303 - val_sparse_categorical_accuracy: 0.7221\n",
            "Epoch 34/100\n",
            "11314/11314 [==============================] - 1s 112us/sample - loss: 0.4142 - sparse_categorical_accuracy: 0.8586 - val_loss: 1.0129 - val_sparse_categorical_accuracy: 0.7258\n",
            "Epoch 35/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.3977 - sparse_categorical_accuracy: 0.8656 - val_loss: 1.0314 - val_sparse_categorical_accuracy: 0.7266\n",
            "Epoch 36/100\n",
            "11314/11314 [==============================] - 1s 111us/sample - loss: 0.4038 - sparse_categorical_accuracy: 0.8598 - val_loss: 1.0359 - val_sparse_categorical_accuracy: 0.7256\n",
            "Epoch 37/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.3969 - sparse_categorical_accuracy: 0.8657 - val_loss: 1.0354 - val_sparse_categorical_accuracy: 0.7296\n",
            "Epoch 38/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.3810 - sparse_categorical_accuracy: 0.8696 - val_loss: 1.0427 - val_sparse_categorical_accuracy: 0.7284\n",
            "Epoch 39/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.3751 - sparse_categorical_accuracy: 0.8741 - val_loss: 1.0432 - val_sparse_categorical_accuracy: 0.7262\n",
            "Epoch 40/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.3719 - sparse_categorical_accuracy: 0.8731 - val_loss: 1.0724 - val_sparse_categorical_accuracy: 0.7323\n",
            "Epoch 41/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.3623 - sparse_categorical_accuracy: 0.8733 - val_loss: 1.0779 - val_sparse_categorical_accuracy: 0.7215\n",
            "Epoch 42/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.3556 - sparse_categorical_accuracy: 0.8756 - val_loss: 1.0777 - val_sparse_categorical_accuracy: 0.7238\n",
            "Epoch 43/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.3505 - sparse_categorical_accuracy: 0.8798 - val_loss: 1.0662 - val_sparse_categorical_accuracy: 0.7281\n",
            "Epoch 44/100\n",
            "11314/11314 [==============================] - 1s 110us/sample - loss: 0.3427 - sparse_categorical_accuracy: 0.8836 - val_loss: 1.0840 - val_sparse_categorical_accuracy: 0.7261\n",
            "Epoch 45/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.3436 - sparse_categorical_accuracy: 0.8816 - val_loss: 1.0945 - val_sparse_categorical_accuracy: 0.7252\n",
            "Epoch 46/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.3369 - sparse_categorical_accuracy: 0.8851 - val_loss: 1.1061 - val_sparse_categorical_accuracy: 0.7253\n",
            "Epoch 47/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.3313 - sparse_categorical_accuracy: 0.8851 - val_loss: 1.0967 - val_sparse_categorical_accuracy: 0.7230\n",
            "Epoch 48/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.3305 - sparse_categorical_accuracy: 0.8847 - val_loss: 1.1078 - val_sparse_categorical_accuracy: 0.7272\n",
            "Epoch 49/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.3157 - sparse_categorical_accuracy: 0.8937 - val_loss: 1.1458 - val_sparse_categorical_accuracy: 0.7229\n",
            "Epoch 50/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.3179 - sparse_categorical_accuracy: 0.8885 - val_loss: 1.1195 - val_sparse_categorical_accuracy: 0.7197\n",
            "Epoch 51/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.3201 - sparse_categorical_accuracy: 0.8867 - val_loss: 1.1322 - val_sparse_categorical_accuracy: 0.7244\n",
            "Epoch 52/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.3097 - sparse_categorical_accuracy: 0.8931 - val_loss: 1.1544 - val_sparse_categorical_accuracy: 0.7276\n",
            "Epoch 53/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2978 - sparse_categorical_accuracy: 0.8932 - val_loss: 1.1576 - val_sparse_categorical_accuracy: 0.7232\n",
            "Epoch 54/100\n",
            "11314/11314 [==============================] - 1s 102us/sample - loss: 0.3109 - sparse_categorical_accuracy: 0.8875 - val_loss: 1.1804 - val_sparse_categorical_accuracy: 0.7227\n",
            "Epoch 55/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2834 - sparse_categorical_accuracy: 0.9018 - val_loss: 1.1803 - val_sparse_categorical_accuracy: 0.7264\n",
            "Epoch 56/100\n",
            "11314/11314 [==============================] - 1s 103us/sample - loss: 0.2980 - sparse_categorical_accuracy: 0.8999 - val_loss: 1.1657 - val_sparse_categorical_accuracy: 0.7284\n",
            "Epoch 57/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2945 - sparse_categorical_accuracy: 0.8994 - val_loss: 1.1812 - val_sparse_categorical_accuracy: 0.7252\n",
            "Epoch 58/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2930 - sparse_categorical_accuracy: 0.8987 - val_loss: 1.1705 - val_sparse_categorical_accuracy: 0.7266\n",
            "Epoch 59/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2813 - sparse_categorical_accuracy: 0.9033 - val_loss: 1.1723 - val_sparse_categorical_accuracy: 0.7300\n",
            "Epoch 60/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2958 - sparse_categorical_accuracy: 0.8976 - val_loss: 1.1659 - val_sparse_categorical_accuracy: 0.7280\n",
            "Epoch 61/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.2801 - sparse_categorical_accuracy: 0.9045 - val_loss: 1.1879 - val_sparse_categorical_accuracy: 0.7246\n",
            "Epoch 62/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2777 - sparse_categorical_accuracy: 0.9038 - val_loss: 1.1956 - val_sparse_categorical_accuracy: 0.7220\n",
            "Epoch 63/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2761 - sparse_categorical_accuracy: 0.9055 - val_loss: 1.2224 - val_sparse_categorical_accuracy: 0.7254\n",
            "Epoch 64/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2609 - sparse_categorical_accuracy: 0.9104 - val_loss: 1.2209 - val_sparse_categorical_accuracy: 0.7300\n",
            "Epoch 65/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2653 - sparse_categorical_accuracy: 0.9065 - val_loss: 1.2504 - val_sparse_categorical_accuracy: 0.7260\n",
            "Epoch 66/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2506 - sparse_categorical_accuracy: 0.9121 - val_loss: 1.2351 - val_sparse_categorical_accuracy: 0.7262\n",
            "Epoch 67/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2741 - sparse_categorical_accuracy: 0.9045 - val_loss: 1.2194 - val_sparse_categorical_accuracy: 0.7256\n",
            "Epoch 68/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2602 - sparse_categorical_accuracy: 0.9091 - val_loss: 1.2568 - val_sparse_categorical_accuracy: 0.7219\n",
            "Epoch 69/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.2584 - sparse_categorical_accuracy: 0.9093 - val_loss: 1.2331 - val_sparse_categorical_accuracy: 0.7224\n",
            "Epoch 70/100\n",
            "11314/11314 [==============================] - 1s 109us/sample - loss: 0.2563 - sparse_categorical_accuracy: 0.9108 - val_loss: 1.2376 - val_sparse_categorical_accuracy: 0.7266\n",
            "Epoch 71/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2487 - sparse_categorical_accuracy: 0.9130 - val_loss: 1.2635 - val_sparse_categorical_accuracy: 0.7228\n",
            "Epoch 72/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2488 - sparse_categorical_accuracy: 0.9159 - val_loss: 1.2402 - val_sparse_categorical_accuracy: 0.7260\n",
            "Epoch 73/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2538 - sparse_categorical_accuracy: 0.9122 - val_loss: 1.2462 - val_sparse_categorical_accuracy: 0.7242\n",
            "Epoch 74/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.2499 - sparse_categorical_accuracy: 0.9128 - val_loss: 1.2682 - val_sparse_categorical_accuracy: 0.7257\n",
            "Epoch 75/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2478 - sparse_categorical_accuracy: 0.9142 - val_loss: 1.2327 - val_sparse_categorical_accuracy: 0.7249\n",
            "Epoch 76/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2424 - sparse_categorical_accuracy: 0.9181 - val_loss: 1.2561 - val_sparse_categorical_accuracy: 0.7244\n",
            "Epoch 77/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.2317 - sparse_categorical_accuracy: 0.9197 - val_loss: 1.2984 - val_sparse_categorical_accuracy: 0.7302\n",
            "Epoch 78/100\n",
            "11314/11314 [==============================] - 1s 103us/sample - loss: 0.2311 - sparse_categorical_accuracy: 0.9196 - val_loss: 1.2755 - val_sparse_categorical_accuracy: 0.7256\n",
            "Epoch 79/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2388 - sparse_categorical_accuracy: 0.9168 - val_loss: 1.3203 - val_sparse_categorical_accuracy: 0.7236\n",
            "Epoch 80/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2372 - sparse_categorical_accuracy: 0.9141 - val_loss: 1.3329 - val_sparse_categorical_accuracy: 0.7237\n",
            "Epoch 81/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2263 - sparse_categorical_accuracy: 0.9218 - val_loss: 1.3179 - val_sparse_categorical_accuracy: 0.7265\n",
            "Epoch 82/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2354 - sparse_categorical_accuracy: 0.9200 - val_loss: 1.2861 - val_sparse_categorical_accuracy: 0.7305\n",
            "Epoch 83/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2333 - sparse_categorical_accuracy: 0.9168 - val_loss: 1.3126 - val_sparse_categorical_accuracy: 0.7278\n",
            "Epoch 84/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.2238 - sparse_categorical_accuracy: 0.9244 - val_loss: 1.3015 - val_sparse_categorical_accuracy: 0.7285\n",
            "Epoch 85/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2319 - sparse_categorical_accuracy: 0.9210 - val_loss: 1.2990 - val_sparse_categorical_accuracy: 0.7270\n",
            "Epoch 86/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2019 - sparse_categorical_accuracy: 0.9319 - val_loss: 1.3185 - val_sparse_categorical_accuracy: 0.7280\n",
            "Epoch 87/100\n",
            "11314/11314 [==============================] - 1s 107us/sample - loss: 0.2279 - sparse_categorical_accuracy: 0.9221 - val_loss: 1.3334 - val_sparse_categorical_accuracy: 0.7256\n",
            "Epoch 88/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.2226 - sparse_categorical_accuracy: 0.9225 - val_loss: 1.3300 - val_sparse_categorical_accuracy: 0.7199\n",
            "Epoch 89/100\n",
            "11314/11314 [==============================] - 1s 105us/sample - loss: 0.2102 - sparse_categorical_accuracy: 0.9262 - val_loss: 1.3675 - val_sparse_categorical_accuracy: 0.7281\n",
            "Epoch 90/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2143 - sparse_categorical_accuracy: 0.9260 - val_loss: 1.3448 - val_sparse_categorical_accuracy: 0.7228\n",
            "Epoch 91/100\n",
            "11314/11314 [==============================] - 1s 109us/sample - loss: 0.2192 - sparse_categorical_accuracy: 0.9243 - val_loss: 1.3696 - val_sparse_categorical_accuracy: 0.7221\n",
            "Epoch 92/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2086 - sparse_categorical_accuracy: 0.9285 - val_loss: 1.4326 - val_sparse_categorical_accuracy: 0.7293\n",
            "Epoch 93/100\n",
            "11314/11314 [==============================] - 1s 103us/sample - loss: 0.2218 - sparse_categorical_accuracy: 0.9237 - val_loss: 1.3836 - val_sparse_categorical_accuracy: 0.7265\n",
            "Epoch 94/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2211 - sparse_categorical_accuracy: 0.9216 - val_loss: 1.3530 - val_sparse_categorical_accuracy: 0.7278\n",
            "Epoch 95/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2155 - sparse_categorical_accuracy: 0.9281 - val_loss: 1.3861 - val_sparse_categorical_accuracy: 0.7293\n",
            "Epoch 96/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2158 - sparse_categorical_accuracy: 0.9249 - val_loss: 1.3512 - val_sparse_categorical_accuracy: 0.7237\n",
            "Epoch 97/100\n",
            "11314/11314 [==============================] - 1s 104us/sample - loss: 0.2074 - sparse_categorical_accuracy: 0.9289 - val_loss: 1.3539 - val_sparse_categorical_accuracy: 0.7249\n",
            "Epoch 98/100\n",
            "11314/11314 [==============================] - 1s 102us/sample - loss: 0.2127 - sparse_categorical_accuracy: 0.9280 - val_loss: 1.3885 - val_sparse_categorical_accuracy: 0.7284\n",
            "Epoch 99/100\n",
            "11314/11314 [==============================] - 1s 108us/sample - loss: 0.1972 - sparse_categorical_accuracy: 0.9299 - val_loss: 1.3808 - val_sparse_categorical_accuracy: 0.7270\n",
            "Epoch 100/100\n",
            "11314/11314 [==============================] - 1s 106us/sample - loss: 0.2024 - sparse_categorical_accuracy: 0.9270 - val_loss: 1.3611 - val_sparse_categorical_accuracy: 0.7302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktQwg2DWet7d",
        "colab_type": "code",
        "outputId": "02ae51b5-530a-4d3d-b4aa-4e9e9ac4984c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh9vqJ-Wd7rN",
        "colab_type": "code",
        "outputId": "dc68dec5-f3fc-4e71-b8e5-551437adc13b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['sparse_categorical_accuracy'], 'r')\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'], 'b')\n",
        "plt.legend(['Training accuracy', 'validation accuracy '])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff73c8b5d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niUwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}