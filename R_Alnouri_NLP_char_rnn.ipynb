{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R. Alnouri - NLP_char_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaghadAlnouri/Deep-learning-projects/blob/master/R_Alnouri_NLP_char_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orvgBiZaAS-S"
      },
      "source": [
        "# Homework: classify the origin of names using a character-level RNN\n",
        "\n",
        "In this homework we will use an rnn-based model to perform classification. The goal is threefold:\n",
        "\n",
        "1. Get more hands on with the preprocessing needed to perform text classification from A to Z. No preprocessing is done for you!\n",
        "2. Use embeddings and RNNs in conjunction at the character level to perform classification.\n",
        "3. Write a function that takes as input a string, and outputs the name of the predicted class.\n",
        "\n",
        "However, here are guidelines to help you through all the steps:\n",
        "\n",
        "1. Figure out the number of classes, and map the classes to integers (or one-hot vectors). This is needed for fitting the model and training it to do classification.\n",
        "2. Use the keras tokenizer at the character level to tokenize your input into integer sequences.\n",
        "3. Pad your sequences using the keras preprocessing tools.\n",
        "4. Build a model that uses, minimally, an embedding layer, an RNN (of your choice) and a dense layer to output the logits or probabilities for the target classes (name origins).\n",
        "5. Fit the model and evaluate on the test set.\n",
        "6. Write a function that takes a string as input and predicts the origin (as its original string value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxpRYUBTCANr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d6cf1c4-6d0e-43aa-a198-2729ef20d170"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzNPPo6_mbjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "87ba6e53-fb12-4eca-acde-6438e0fb9d8a"
      },
      "source": [
        "# install missing package\n",
        "!pip install unidecode\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 32.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 8.5MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF1BqaO7-A2g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "9e341ab9-3496-439d-f324-8afa1b32ec60"
      },
      "source": [
        "# Download the data\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-23 01:22:18--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.227.198.24, 13.227.198.30, 13.227.198.106, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.227.198.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-03-23 01:22:18 (91.6 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz-k11YhGxa2"
      },
      "source": [
        "Building the category_lines dictionary, a list of names per language\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnj5MDEJ-u-q"
      },
      "source": [
        "data = []\n",
        "for filename in glob('data/names/*.txt'):\n",
        "  origin = filename.split('/')[-1].split('.txt')[0]\n",
        "  names = open(filename).readlines()\n",
        "  for name in names:\n",
        "    data.append((name.strip(), origin))\n",
        "\n",
        "names, origins = zip(*data)\n",
        "names_train, names_test, origins_train, origins_test = train_test_split(names, origins, test_size=0.25, shuffle=True, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUhDpvskAHZ1"
      },
      "source": [
        "# Lets look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ux9M4DV-A5s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "44a76996-6fb2-418e-dc17-212f530e1130"
      },
      "source": [
        "for name, origin in zip(names_train[:10], origins_train[:10]):\n",
        "  print(name.ljust(10), origin)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yusuf      English\n",
            "Vikhrev    Russian\n",
            "Ilyuhin    Russian\n",
            "Paterson   English\n",
            "Abt        German\n",
            "Koury      Arabic\n",
            "Mihalkin   Russian\n",
            "Favre      French\n",
            "Frolkov    Russian\n",
            "Agabekov   Russian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh9CAtXEI4Gj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6557e5e-7fa7-4c2b-91bb-90e7147a8230"
      },
      "source": [
        "len(names_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15055"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gklIuIxkI9Dr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e19ff52e-360a-4b38-cdfa-3a4bc8bb9f63"
      },
      "source": [
        "print (names_train[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Yusuf', 'Vikhrev', 'Ilyuhin', 'Paterson', 'Abt', 'Koury', 'Mihalkin', 'Favre', 'Frolkov', 'Agabekov', 'Awturkhanoff', 'Aglinskas', 'Jigailov', 'Vallah', 'Verber', 'Bishara', 'Amari', 'Onoda', 'Viola', 'Hlopkin', 'Lind', 'Baturov', 'Farrow', 'Veletsky', 'Djabrailov', 'Yablonovsky', 'Kikutake', 'Kalihov', 'Tubolkin', 'Mikhail', 'Fearghal', 'Borovski', 'Mills', 'Balandyuk', 'Schoettmer', 'Yushkevich', 'Sha', 'Balazowski', 'Padovano', 'Kilshaw', 'Agranovitch', 'Desmond', 'Orwin', 'Cameron', 'Wiggins', 'Leys', 'Aboimov', 'Xiang', 'Lumley', 'Xun', 'Bakhelov', 'Nijo', 'Ricci', 'Velikanov', 'Naifeh', 'Destunis', 'Sakoda', 'Horn', 'Said', 'Aihara', 'Dagher', 'Grout', 'Quraishi', 'Fischer', 'King', 'Garofalo', 'Alesci', 'Gladilin', 'Zasetsky', 'Jivoluk', 'Nahmanovich', 'Zheltov', 'Bui', 'Dubrovo', 'Agapiev', 'Yanovka', 'Ko', 'Becke', 'Granger', 'Shionoya', 'Zherebin', 'Maher', 'Fricker', 'Muhlfeld', 'Yamawaki', 'Asghar', 'Agrachev', 'Agafonoff', 'Kram', 'Sakiyaev', 'Goldman', 'an', 'Kachemaev', 'Jamussa', 'Reinder', 'Dobronos', 'Kahae', 'Ots', 'Corna', 'Aiello']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2f1TCmyDcIF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42e53b3e-efa1-42b5-b320-3711ecce31ac"
      },
      "source": [
        "# Build the category_lines dictionary, a list of names per language\n",
        "all_categories=list(set(origins))\n",
        "n_categories = len(all_categories)\n",
        "n_categories"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaFnORiSE6Ls",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "32b38c31-e686-44f6-f0d9-6784b88cc798"
      },
      "source": [
        "for i, c in enumerate(all_categories):\n",
        "  print (i,\":\",c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 : Irish\n",
            "1 : Scottish\n",
            "2 : Czech\n",
            "3 : French\n",
            "4 : Chinese\n",
            "5 : Vietnamese\n",
            "6 : German\n",
            "7 : Portuguese\n",
            "8 : Dutch\n",
            "9 : Arabic\n",
            "10 : Russian\n",
            "11 : Italian\n",
            "12 : Polish\n",
            "13 : Greek\n",
            "14 : Japanese\n",
            "15 : Spanish\n",
            "16 : English\n",
            "17 : Korean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFn6qyQ_xrCs"
      },
      "source": [
        "y_train = [all_categories.index(o) for o in origins_train]\n",
        "y_test = [all_categories.index(o) for o in origins_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPnG2tDzyIWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18857f48-af76-40f0-a3c3-bb51e7271288"
      },
      "source": [
        "y_train[0], origins_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 'English')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbt94FF3yO02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5011c5c4-0084-466a-d024-872db23d04f1"
      },
      "source": [
        "len(y_train), len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15055, 5019)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLZt-7GuOWte"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_train, y_test = np.asarray([all_categories.index(s) for s in origins_train]), np.asarray([all_categories.index(s) for s in origins_test])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrZBU08C7Ma"
      },
      "source": [
        "Preparing the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbriVUIp6vJb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a395f70-37a1-4ade-eb5a-c41f6c545160"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII\n",
        "\n",
        "all_letters = '0' + string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "import unicodedata\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "  \n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Slusarski\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34V3miMVuZts",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b83983e4-3791-43cc-f967-9158e7bb2098"
      },
      "source": [
        "names_train[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yusuf',\n",
              " 'Vikhrev',\n",
              " 'Ilyuhin',\n",
              " 'Paterson',\n",
              " 'Abt',\n",
              " 'Koury',\n",
              " 'Mihalkin',\n",
              " 'Favre',\n",
              " 'Frolkov',\n",
              " 'Agabekov']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC-5mWPJpE3f"
      },
      "source": [
        "X_train_clean = [unicodeToAscii(x) for x in names_train]\n",
        "X_test_clean = [unicodeToAscii(x) for x in names_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWCLqODRuTev",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8eb831d2-3c79-464c-a93e-0d6b49396293"
      },
      "source": [
        "X_train_clean[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yusuf',\n",
              " 'Vikhrev',\n",
              " 'Ilyuhin',\n",
              " 'Paterson',\n",
              " 'Abt',\n",
              " 'Koury',\n",
              " 'Mihalkin',\n",
              " 'Favre',\n",
              " 'Frolkov',\n",
              " 'Agabekov']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA9cYHGl5lEw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a01e777-c4b6-451a-9425-1d4e62e3ee1b"
      },
      "source": [
        "n_categories"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZH0AHv44CDx"
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "\n",
        "# compute max_len of sentences in train data\n",
        "def compute_max_len(data):\n",
        "  max_n = 1\n",
        "  for i, sent in enumerate(data):\n",
        "    if len(sent) > max_n:\n",
        "      max_n = len(sent)\n",
        "  return max_n\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 1, \"0\" = 0 (padding value)\n",
        "def letterToIndex(letter):\n",
        "    ind = all_letters.find(letter)\n",
        "    if ind < 0:\n",
        "      raise Exception('unknown letter:' + letter)\n",
        "    return ind\n",
        "\n",
        "# Turn a line into an array of one-hot letter vectors\n",
        "def lineToTensor(line, max_n):\n",
        "    tensor = np.zeros((max_n, n_letters))\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][letterToIndex(letter)] = 1\n",
        "    if len(line) < max_n:\n",
        "      tensor[len(line):][0] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into an array of indices\n",
        "def lineToIndex(line, max_n):\n",
        "    tensor = np.zeros(max_n, dtype=int)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li] = letterToIndex(letter)\n",
        "    return tensor\n",
        "\n",
        "max_len = max(compute_max_len(X_train_clean), compute_max_len(X_test_clean))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2sgpQCQEWUQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "552ff24e-c179-4599-b98f-73b45c9082b9"
      },
      "source": [
        "max_len\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1RhRESJ5_Qg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb195960-235b-46c0-e5a8-504515563ad2"
      },
      "source": [
        "\n",
        "# lineToTensor('Jones', max_len).shape\n",
        "print(lineToIndex('Joanes', max_len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36 15  1 14  5 19  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDGCCd19oWeL"
      },
      "source": [
        "# data_train = np.array([lineToTensor(x, max_len) for x in X_train_clean])\n",
        "# data_test =  np.array([lineToTensor(x, max_len) for x in X_test_clean])\n",
        "data_train = np.array([lineToIndex(x, max_len) for x in X_train_clean])\n",
        "data_test =  np.array([lineToIndex(x, max_len) for x in X_test_clean])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiRB-our-p8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac7e1e94-79a5-45d2-c999-7b2cee0ccd21"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15055, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLJDtfOdXXEH"
      },
      "source": [
        "Building the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aJ_o8k3AwI6"
      },
      "source": [
        "\n",
        "model = tf.keras.models.Sequential(layers=[\n",
        "   tf.keras.layers.Embedding(input_dim=n_letters,\n",
        "                            output_dim=32, mask_zero=True),\n",
        "  tf.keras.layers.LSTM(units=64, return_sequences=True), \n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(n_categories, activation='softmax')\n",
        "])\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(lr=0.005), metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCLYgrAiIKWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a69143c7-dcc9-471d-9d6f-4c9eafb5f1a2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 32)          1856      \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, None, 64)          24832     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_8 ( (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 18)                1170      \n",
            "=================================================================\n",
            "Total params: 27,858\n",
            "Trainable params: 27,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpK7vJEEYZpn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02563923-654b-4c20-b53f-d8829eb35e86"
      },
      "source": [
        "# for i in range(0, len(data_train), 32):\n",
        "#   #print(i)\n",
        "#   out = model.predict(data_train[i:i+32])\n",
        "#   #print(out.shape)\n",
        "\n",
        "history = model.fit(data_train, y_train, validation_data=(data_test, y_test), batch_size=32, shuffle=True, epochs=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15055 samples, validate on 5019 samples\n",
            "Epoch 1/30\n",
            "15055/15055 [==============================] - 4s 251us/sample - loss: 0.3578 - accuracy: 0.8878 - val_loss: 0.6342 - val_accuracy: 0.8155\n",
            "Epoch 2/30\n",
            "15055/15055 [==============================] - 4s 241us/sample - loss: 0.3360 - accuracy: 0.8969 - val_loss: 0.6497 - val_accuracy: 0.8133\n",
            "Epoch 3/30\n",
            "15055/15055 [==============================] - 4s 238us/sample - loss: 0.3160 - accuracy: 0.9022 - val_loss: 0.6438 - val_accuracy: 0.8149\n",
            "Epoch 4/30\n",
            "15055/15055 [==============================] - 4s 250us/sample - loss: 0.2971 - accuracy: 0.9048 - val_loss: 0.6565 - val_accuracy: 0.8141\n",
            "Epoch 5/30\n",
            "15055/15055 [==============================] - 4s 246us/sample - loss: 0.2829 - accuracy: 0.9099 - val_loss: 0.6657 - val_accuracy: 0.8119\n",
            "Epoch 6/30\n",
            "15055/15055 [==============================] - 4s 255us/sample - loss: 0.2708 - accuracy: 0.9127 - val_loss: 0.6589 - val_accuracy: 0.8187\n",
            "Epoch 7/30\n",
            "15055/15055 [==============================] - 4s 238us/sample - loss: 0.2505 - accuracy: 0.9232 - val_loss: 0.6716 - val_accuracy: 0.8125\n",
            "Epoch 8/30\n",
            "15055/15055 [==============================] - 4s 239us/sample - loss: 0.2397 - accuracy: 0.9258 - val_loss: 0.6999 - val_accuracy: 0.8097\n",
            "Epoch 9/30\n",
            "15055/15055 [==============================] - 4s 250us/sample - loss: 0.2297 - accuracy: 0.9288 - val_loss: 0.7106 - val_accuracy: 0.8133\n",
            "Epoch 10/30\n",
            "15055/15055 [==============================] - 4s 244us/sample - loss: 0.2244 - accuracy: 0.9277 - val_loss: 0.7001 - val_accuracy: 0.8161\n",
            "Epoch 11/30\n",
            "15055/15055 [==============================] - 4s 250us/sample - loss: 0.2130 - accuracy: 0.9298 - val_loss: 0.7189 - val_accuracy: 0.8125\n",
            "Epoch 12/30\n",
            "15055/15055 [==============================] - 4s 242us/sample - loss: 0.2055 - accuracy: 0.9344 - val_loss: 0.7209 - val_accuracy: 0.8123\n",
            "Epoch 13/30\n",
            "15055/15055 [==============================] - 4s 240us/sample - loss: 0.1984 - accuracy: 0.9376 - val_loss: 0.7256 - val_accuracy: 0.8097\n",
            "Epoch 14/30\n",
            "15055/15055 [==============================] - 4s 241us/sample - loss: 0.1901 - accuracy: 0.9421 - val_loss: 0.7436 - val_accuracy: 0.8135\n",
            "Epoch 15/30\n",
            "15055/15055 [==============================] - 4s 236us/sample - loss: 0.1852 - accuracy: 0.9404 - val_loss: 0.7490 - val_accuracy: 0.8099\n",
            "Epoch 16/30\n",
            "15055/15055 [==============================] - 4s 236us/sample - loss: 0.1756 - accuracy: 0.9443 - val_loss: 0.7782 - val_accuracy: 0.8057\n",
            "Epoch 17/30\n",
            "15055/15055 [==============================] - 4s 247us/sample - loss: 0.1817 - accuracy: 0.9411 - val_loss: 0.7635 - val_accuracy: 0.8069\n",
            "Epoch 18/30\n",
            "15055/15055 [==============================] - 4s 241us/sample - loss: 0.1768 - accuracy: 0.9421 - val_loss: 0.7688 - val_accuracy: 0.8053\n",
            "Epoch 19/30\n",
            "15055/15055 [==============================] - 4s 249us/sample - loss: 0.1657 - accuracy: 0.9456 - val_loss: 0.7691 - val_accuracy: 0.8109\n",
            "Epoch 20/30\n",
            "15055/15055 [==============================] - 4s 242us/sample - loss: 0.1629 - accuracy: 0.9451 - val_loss: 0.7943 - val_accuracy: 0.8105\n",
            "Epoch 21/30\n",
            "15055/15055 [==============================] - 4s 248us/sample - loss: 0.1545 - accuracy: 0.9485 - val_loss: 0.8012 - val_accuracy: 0.8097\n",
            "Epoch 22/30\n",
            "15055/15055 [==============================] - 4s 247us/sample - loss: 0.1535 - accuracy: 0.9495 - val_loss: 0.8371 - val_accuracy: 0.8029\n",
            "Epoch 23/30\n",
            "15055/15055 [==============================] - 4s 244us/sample - loss: 0.1554 - accuracy: 0.9477 - val_loss: 0.8118 - val_accuracy: 0.8091\n",
            "Epoch 24/30\n",
            "15055/15055 [==============================] - 4s 244us/sample - loss: 0.1466 - accuracy: 0.9504 - val_loss: 0.8385 - val_accuracy: 0.8071\n",
            "Epoch 25/30\n",
            "15055/15055 [==============================] - 4s 245us/sample - loss: 0.1403 - accuracy: 0.9536 - val_loss: 0.8395 - val_accuracy: 0.8105\n",
            "Epoch 26/30\n",
            "15055/15055 [==============================] - 4s 247us/sample - loss: 0.1431 - accuracy: 0.9510 - val_loss: 0.8509 - val_accuracy: 0.8067\n",
            "Epoch 27/30\n",
            "15055/15055 [==============================] - 4s 235us/sample - loss: 0.1496 - accuracy: 0.9502 - val_loss: 0.8415 - val_accuracy: 0.8061\n",
            "Epoch 28/30\n",
            "15055/15055 [==============================] - 4s 249us/sample - loss: 0.1387 - accuracy: 0.9521 - val_loss: 0.8588 - val_accuracy: 0.8035\n",
            "Epoch 29/30\n",
            "15055/15055 [==============================] - 4s 246us/sample - loss: 0.1364 - accuracy: 0.9514 - val_loss: 0.8684 - val_accuracy: 0.8123\n",
            "Epoch 30/30\n",
            "15055/15055 [==============================] - 4s 246us/sample - loss: 0.1403 - accuracy: 0.9506 - val_loss: 0.8824 - val_accuracy: 0.8075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCY2alZ2PJgv"
      },
      "source": [
        "def predict_origin(name):\n",
        "  assert isinstance(name, str)\n",
        "  the_origin = None\n",
        "  x = np.array(lineToIndex(name, max_len))\n",
        "  output = model.predict(x.reshape(1,-1))\n",
        "  # print(output)\n",
        "  the_origin = np.argmax(output)\n",
        "  # print(the_origin)\n",
        "  return all_categories[the_origin]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqGQ1ndSXu3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ef79fdc-be86-4a42-9e7c-5474b6ede7d7"
      },
      "source": [
        "predict_origin('Alnouri')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Arabic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT6bid0qcOLu"
      },
      "source": [
        "Successfully predicted my origin :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOqK-7fIcSyu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}